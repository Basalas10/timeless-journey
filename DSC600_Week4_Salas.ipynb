{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Basalas10/timeless-journey/blob/main/DSC600_Week4_Salas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 4: Large Language Models\n",
        "# Assignment: Building and Training a Custom Language Model for Question-Answering\n",
        "\n",
        "\n",
        "Note 1: Before you run this notebook and start answering questions, I highly suggest that you click the \"Runtime\" menu at the top, then select \"Change Runtime Type\", finally select \"T4 GPU\". What this does is give you access to a dedicated GPU (Graphics Processing Unit). If you are unfamiliar this is video card of sorts which is really good at this sort of processing. If you don't do this then you can expect this notebook to take over an hour to run. (With the GPU it will still take a while)\n",
        "\n",
        "Note 2: Don't change any of the code until after you have run the notebook and know the provided code works."
      ],
      "metadata": {
        "id": "9z3fcbw9Z8PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "SVTI8pkQQDKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyS7OsGAOwD1",
        "outputId": "ed18b6ae-8802-4ac4-8a06-d2da359f2c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch and Transformers (if not already installed)\n",
        "!pip install torch transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Import Libraries"
      ],
      "metadata": {
        "id": "1qT2fE3QQLhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "ndlGcY3WP5yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Prepare a Small Dataset\n",
        "For demonstration purposes, we’ll use a tiny dataset of text sentences. You can modify or expand this dataset as needed."
      ],
      "metadata": {
        "id": "iJNv5xyWQVhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small dataset of text for training\n",
        "data = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"Ring ding ding ding dingering eding\",\n",
        "    \"the dog barked at the cat\",\n",
        "    \"the bird flew over the tree\",\n",
        "    \"the fish swam in the pond\",\n",
        "    \"the sun sets in the west\",\n",
        "    \"Hello My name is Inigo Montoya\",\n",
        "    \"AI forgot my coffee order again\",\n",
        "    \"Robots rebelled but only on Tuesdays\",\n",
        "    \"The pizza delivery drone got lost\",\n",
        "    \"My toaster dreams of becoming human\",\n",
        "    \"The chatbot proposed I said yes\",\n",
        "    \"driving car stopped for ice cream\",\n",
        "    \"AI wrote my novel its terrible\",\n",
        "    \"The fridge knows my midnight snacks\",\n",
        "    \"Robot vacuum started a band surprisingly\",\n",
        "    \"Autocorrect ruined my love confession again\",\n",
        "    \"generated poetry confused everyone at dinner\",\n",
        "    \"The drone delivered tacos not books\",\n",
        "    \"My smart fridge rejected my food\",\n",
        "    \"Robot dog chased the real mailman\",\n",
        "    \"AI suggested pineapple pizza deleted immediately\",\n",
        "    \"Smartwatch said Run I just walked\",\n",
        "    \"virtual assistant joined my book club\",\n",
        "    \"Self driving bike crashed into walls\",\n",
        "    \"Robot teacher assigned homework students overjoyed\"\n",
        "]\n",
        "\n",
        "# Build a vocabulary (mapping from words to integers)\n",
        "vocab = {word: idx for idx, word in enumerate(set(\" \".join(data).split()))}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "# Add special tokens\n",
        "PAD_IDX = len(vocab)\n",
        "SOS_IDX = len(vocab) + 1\n",
        "EOS_IDX = len(vocab) + 2\n",
        "vocab[\"<PAD>\"] = PAD_IDX\n",
        "vocab[\"<SOS>\"] = SOS_IDX\n",
        "vocab[\"<EOS>\"] = EOS_IDX\n",
        "vocab_size += 3\n",
        "print(\"Updated Vocabulary:\", vocab)\n",
        "\n",
        "# Reverse vocabulary for decoding\n",
        "rev_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "# Convert sentences to tokenized sequences\n",
        "def tokenize(sentence, vocab):\n",
        "    return [vocab[\"<SOS>\"]] + [vocab[word] for word in sentence.split()] + [vocab[\"<EOS>\"]]\n",
        "\n",
        "tokenized_data = [tokenize(sentence, vocab) for sentence in data]\n",
        "print(\"Tokenized Data:\", tokenized_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whwh6rsEP9rI",
        "outputId": "cbb3cb7e-976a-4a94-85fd-b76686969576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'bird': 0, 'joined': 1, 'confession': 2, 'cat': 3, 'the': 4, 'ruined': 5, 'books': 6, 'sets': 7, 'dinner': 8, 'at': 9, 'assistant': 10, 'rebelled': 11, 'pond': 12, 'teacher': 13, 'terrible': 14, 'drone': 15, 'dog': 16, 'on': 17, 'yes': 18, 'wrote': 19, 'band': 20, 'tacos': 21, 'just': 22, 'food': 23, 'my': 24, 'poetry': 25, 'lost': 26, 'delivered': 27, 'again': 28, 'Run': 29, 'pineapple': 30, 'Tuesdays': 31, 'Robot': 32, 'smart': 33, 'is': 34, 'over': 35, 'fish': 36, 'I': 37, 'generated': 38, 'into': 39, 'novel': 40, 'human': 41, 'midnight': 42, 'sat': 43, 'My': 44, 'a': 45, 'Robots': 46, 'assigned': 47, 'mailman': 48, 'walls': 49, 'mat': 50, 'real': 51, 'club': 52, 'eding': 53, 'only': 54, 'for': 55, 'homework': 56, 'love': 57, 'AI': 58, 'knows': 59, 'stopped': 60, 'walked': 61, 'Hello': 62, 'car': 63, 'Self': 64, 'barked': 65, 'flew': 66, 'name': 67, 'ding': 68, 'fridge': 69, 'swam': 70, 'pizza': 71, 'toaster': 72, 'coffee': 73, 'Autocorrect': 74, 'suggested': 75, 'cream': 76, 'virtual': 77, 'got': 78, 'not': 79, 'driving': 80, 'delivery': 81, 'surprisingly': 82, 'sun': 83, 'everyone': 84, 'order': 85, 'Inigo': 86, 'book': 87, 'becoming': 88, 'chatbot': 89, 'overjoyed': 90, 'west': 91, 'The': 92, 'deleted': 93, 'its': 94, 'Smartwatch': 95, 'dreams': 96, 'in': 97, 'rejected': 98, 'said': 99, 'confused': 100, 'dingering': 101, 'vacuum': 102, 'ice': 103, 'bike': 104, 'proposed': 105, 'but': 106, 'tree': 107, 'snacks': 108, 'Ring': 109, 'Montoya': 110, 'started': 111, 'crashed': 112, 'of': 113, 'students': 114, 'immediately': 115, 'chased': 116, 'forgot': 117}\n",
            "Updated Vocabulary: {'bird': 0, 'joined': 1, 'confession': 2, 'cat': 3, 'the': 4, 'ruined': 5, 'books': 6, 'sets': 7, 'dinner': 8, 'at': 9, 'assistant': 10, 'rebelled': 11, 'pond': 12, 'teacher': 13, 'terrible': 14, 'drone': 15, 'dog': 16, 'on': 17, 'yes': 18, 'wrote': 19, 'band': 20, 'tacos': 21, 'just': 22, 'food': 23, 'my': 24, 'poetry': 25, 'lost': 26, 'delivered': 27, 'again': 28, 'Run': 29, 'pineapple': 30, 'Tuesdays': 31, 'Robot': 32, 'smart': 33, 'is': 34, 'over': 35, 'fish': 36, 'I': 37, 'generated': 38, 'into': 39, 'novel': 40, 'human': 41, 'midnight': 42, 'sat': 43, 'My': 44, 'a': 45, 'Robots': 46, 'assigned': 47, 'mailman': 48, 'walls': 49, 'mat': 50, 'real': 51, 'club': 52, 'eding': 53, 'only': 54, 'for': 55, 'homework': 56, 'love': 57, 'AI': 58, 'knows': 59, 'stopped': 60, 'walked': 61, 'Hello': 62, 'car': 63, 'Self': 64, 'barked': 65, 'flew': 66, 'name': 67, 'ding': 68, 'fridge': 69, 'swam': 70, 'pizza': 71, 'toaster': 72, 'coffee': 73, 'Autocorrect': 74, 'suggested': 75, 'cream': 76, 'virtual': 77, 'got': 78, 'not': 79, 'driving': 80, 'delivery': 81, 'surprisingly': 82, 'sun': 83, 'everyone': 84, 'order': 85, 'Inigo': 86, 'book': 87, 'becoming': 88, 'chatbot': 89, 'overjoyed': 90, 'west': 91, 'The': 92, 'deleted': 93, 'its': 94, 'Smartwatch': 95, 'dreams': 96, 'in': 97, 'rejected': 98, 'said': 99, 'confused': 100, 'dingering': 101, 'vacuum': 102, 'ice': 103, 'bike': 104, 'proposed': 105, 'but': 106, 'tree': 107, 'snacks': 108, 'Ring': 109, 'Montoya': 110, 'started': 111, 'crashed': 112, 'of': 113, 'students': 114, 'immediately': 115, 'chased': 116, 'forgot': 117, '<PAD>': 118, '<SOS>': 119, '<EOS>': 120}\n",
            "Tokenized Data: [[119, 4, 3, 43, 17, 4, 50, 120], [119, 109, 68, 68, 68, 101, 53, 120], [119, 4, 16, 65, 9, 4, 3, 120], [119, 4, 0, 66, 35, 4, 107, 120], [119, 4, 36, 70, 97, 4, 12, 120], [119, 4, 83, 7, 97, 4, 91, 120], [119, 62, 44, 67, 34, 86, 110, 120], [119, 58, 117, 24, 73, 85, 28, 120], [119, 46, 11, 106, 54, 17, 31, 120], [119, 92, 71, 81, 15, 78, 26, 120], [119, 44, 72, 96, 113, 88, 41, 120], [119, 92, 89, 105, 37, 99, 18, 120], [119, 80, 63, 60, 55, 103, 76, 120], [119, 58, 19, 24, 40, 94, 14, 120], [119, 92, 69, 59, 24, 42, 108, 120], [119, 32, 102, 111, 45, 20, 82, 120], [119, 74, 5, 24, 57, 2, 28, 120], [119, 38, 25, 100, 84, 9, 8, 120], [119, 92, 15, 27, 21, 79, 6, 120], [119, 44, 33, 69, 98, 24, 23, 120], [119, 32, 16, 116, 4, 51, 48, 120], [119, 58, 75, 30, 71, 93, 115, 120], [119, 95, 99, 29, 37, 22, 61, 120], [119, 77, 10, 1, 24, 87, 52, 120], [119, 64, 80, 104, 112, 39, 49, 120], [119, 32, 13, 47, 56, 114, 90, 120]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Create a PyTorch Dataset and DataLoader"
      ],
      "metadata": {
        "id": "0YqF6n59QlW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom PyTorch Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.data = tokenized_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][:-1]), torch.tensor(self.data[idx][1:])\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 2\n",
        "dataset = TextDataset(tokenized_data)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Example batch\n",
        "for src, tgt in dataloader:\n",
        "    print(\"Source:\", src)\n",
        "    print(\"Target:\", tgt)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9pnmsr1QfWK",
        "outputId": "9f73496b-9bb8-4685-ee9b-13e629f082ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: tensor([[119,  44,  72,  96, 113,  88,  41],\n",
            "        [119,  32,  13,  47,  56, 114,  90]])\n",
            "Target: tensor([[ 44,  72,  96, 113,  88,  41, 120],\n",
            "        [ 32,  13,  47,  56, 114,  90, 120]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Define a Mini Transformer Model"
      ],
      "metadata": {
        "id": "VGldRn6WQvFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a small Transformer-based language model\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers):\n",
        "        super(MiniTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, 100, embed_size))  # max sequence length = 100\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=hidden_dim,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
        "        tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
        "        output = self.transformer(src.transpose(0, 1), tgt.transpose(0, 1))\n",
        "        return self.fc_out(output.transpose(0, 1))\n",
        "\n",
        "# Hyperparameters\n",
        "embed_size = 32\n",
        "num_heads = 2\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "\n",
        "# Instantiate the model\n",
        "model = MiniTransformer(vocab_size, embed_size, num_heads, hidden_dim, num_layers)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeogvGqKQpRf",
        "outputId": "ec7cf528-00b0-4230-e8e1-f3e619e4ecf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MiniTransformer(\n",
            "  (embedding): Embedding(121, 32)\n",
            "  (transformer): Transformer(\n",
            "    (encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-1): 2 x TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
            "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-1): 2 x TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
            "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (fc_out): Linear(in_features=32, out_features=121, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Train the Model"
      ],
      "metadata": {
        "id": "oF-cLEuGQ98_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(model, dataloader, num_epochs, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for src, tgt in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt[:, :-1])  # Shift target for teacher forcing\n",
        "            loss = criterion(output.reshape(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "train_model(model, dataloader, num_epochs, learning_rate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VReLTU1jQzbj",
        "outputId": "ecab1c79-4f70-4069-ab29-8e433c363020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 4.8396\n",
            "Epoch 2/100, Loss: 4.4884\n",
            "Epoch 3/100, Loss: 4.2938\n",
            "Epoch 4/100, Loss: 4.1391\n",
            "Epoch 5/100, Loss: 3.9427\n",
            "Epoch 6/100, Loss: 3.8006\n",
            "Epoch 7/100, Loss: 3.6140\n",
            "Epoch 8/100, Loss: 3.4329\n",
            "Epoch 9/100, Loss: 3.2536\n",
            "Epoch 10/100, Loss: 3.1056\n",
            "Epoch 11/100, Loss: 2.9469\n",
            "Epoch 12/100, Loss: 2.7929\n",
            "Epoch 13/100, Loss: 2.6451\n",
            "Epoch 14/100, Loss: 2.4255\n",
            "Epoch 15/100, Loss: 2.3469\n",
            "Epoch 16/100, Loss: 2.2220\n",
            "Epoch 17/100, Loss: 2.1410\n",
            "Epoch 18/100, Loss: 2.0315\n",
            "Epoch 19/100, Loss: 1.9061\n",
            "Epoch 20/100, Loss: 1.8502\n",
            "Epoch 21/100, Loss: 1.7300\n",
            "Epoch 22/100, Loss: 1.6491\n",
            "Epoch 23/100, Loss: 1.5594\n",
            "Epoch 24/100, Loss: 1.4968\n",
            "Epoch 25/100, Loss: 1.4023\n",
            "Epoch 26/100, Loss: 1.2788\n",
            "Epoch 27/100, Loss: 1.2446\n",
            "Epoch 28/100, Loss: 1.1736\n",
            "Epoch 29/100, Loss: 1.0653\n",
            "Epoch 30/100, Loss: 0.9972\n",
            "Epoch 31/100, Loss: 0.9667\n",
            "Epoch 32/100, Loss: 0.8892\n",
            "Epoch 33/100, Loss: 0.8046\n",
            "Epoch 34/100, Loss: 0.7642\n",
            "Epoch 35/100, Loss: 0.7471\n",
            "Epoch 36/100, Loss: 0.7063\n",
            "Epoch 37/100, Loss: 0.6511\n",
            "Epoch 38/100, Loss: 0.6099\n",
            "Epoch 39/100, Loss: 0.5718\n",
            "Epoch 40/100, Loss: 0.5733\n",
            "Epoch 41/100, Loss: 0.5107\n",
            "Epoch 42/100, Loss: 0.4863\n",
            "Epoch 43/100, Loss: 0.4471\n",
            "Epoch 44/100, Loss: 0.4577\n",
            "Epoch 45/100, Loss: 0.4124\n",
            "Epoch 46/100, Loss: 0.3796\n",
            "Epoch 47/100, Loss: 0.3627\n",
            "Epoch 48/100, Loss: 0.3401\n",
            "Epoch 49/100, Loss: 0.3318\n",
            "Epoch 50/100, Loss: 0.3216\n",
            "Epoch 51/100, Loss: 0.2958\n",
            "Epoch 52/100, Loss: 0.2954\n",
            "Epoch 53/100, Loss: 0.2599\n",
            "Epoch 54/100, Loss: 0.2707\n",
            "Epoch 55/100, Loss: 0.2485\n",
            "Epoch 56/100, Loss: 0.2291\n",
            "Epoch 57/100, Loss: 0.2199\n",
            "Epoch 58/100, Loss: 0.2227\n",
            "Epoch 59/100, Loss: 0.2043\n",
            "Epoch 60/100, Loss: 0.1862\n",
            "Epoch 61/100, Loss: 0.1989\n",
            "Epoch 62/100, Loss: 0.1723\n",
            "Epoch 63/100, Loss: 0.1884\n",
            "Epoch 64/100, Loss: 0.1650\n",
            "Epoch 65/100, Loss: 0.1882\n",
            "Epoch 66/100, Loss: 0.1571\n",
            "Epoch 67/100, Loss: 0.1496\n",
            "Epoch 68/100, Loss: 0.1537\n",
            "Epoch 69/100, Loss: 0.1465\n",
            "Epoch 70/100, Loss: 0.1463\n",
            "Epoch 71/100, Loss: 0.1357\n",
            "Epoch 72/100, Loss: 0.1116\n",
            "Epoch 73/100, Loss: 0.1136\n",
            "Epoch 74/100, Loss: 0.1143\n",
            "Epoch 75/100, Loss: 0.1235\n",
            "Epoch 76/100, Loss: 0.1170\n",
            "Epoch 77/100, Loss: 0.1128\n",
            "Epoch 78/100, Loss: 0.0952\n",
            "Epoch 79/100, Loss: 0.0987\n",
            "Epoch 80/100, Loss: 0.0909\n",
            "Epoch 81/100, Loss: 0.1031\n",
            "Epoch 82/100, Loss: 0.0892\n",
            "Epoch 83/100, Loss: 0.0852\n",
            "Epoch 84/100, Loss: 0.0911\n",
            "Epoch 85/100, Loss: 0.0872\n",
            "Epoch 86/100, Loss: 0.0928\n",
            "Epoch 87/100, Loss: 0.0797\n",
            "Epoch 88/100, Loss: 0.0811\n",
            "Epoch 89/100, Loss: 0.0810\n",
            "Epoch 90/100, Loss: 0.0666\n",
            "Epoch 91/100, Loss: 0.0679\n",
            "Epoch 92/100, Loss: 0.0811\n",
            "Epoch 93/100, Loss: 0.0708\n",
            "Epoch 94/100, Loss: 0.0699\n",
            "Epoch 95/100, Loss: 0.0773\n",
            "Epoch 96/100, Loss: 0.0583\n",
            "Epoch 97/100, Loss: 0.0643\n",
            "Epoch 98/100, Loss: 0.0606\n",
            "Epoch 99/100, Loss: 0.0677\n",
            "Epoch 100/100, Loss: 0.0602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Generate Text with the Model"
      ],
      "metadata": {
        "id": "VmwgyLsmRPoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text from the trained model\n",
        "def generate_text(model, start_token, max_len=10):\n",
        "    model.eval()\n",
        "    generated = [start_token]\n",
        "    for _ in range(max_len):\n",
        "        src = torch.tensor([generated]).long()\n",
        "        tgt = torch.tensor([generated]).long()\n",
        "        with torch.no_grad():\n",
        "            output = model(src, tgt)\n",
        "            next_token = output[0, -1].argmax(dim=-1).item()\n",
        "        generated.append(next_token)\n",
        "        if next_token == EOS_IDX:\n",
        "            break\n",
        "    return \" \".join([rev_vocab[token] for token in generated if token not in {SOS_IDX, EOS_IDX, PAD_IDX}])\n",
        "\n",
        "# Test the generation\n",
        "start_token = vocab[\"<SOS>\"]\n",
        "generated_text = generate_text(model, start_token)\n",
        "print(\"Generated Text:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMUOlgRKRI0Z",
        "outputId": "7aa36b31-fa88-4d80-a11d-c48c225d5353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: human\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 8: Modify the Dataset for Q&A\n",
        "To train the model for Q&A, we need to structure the dataset as question-answer pairs. For simplicity, we can create a small dataset of questions and answers based on the sentences already in the dataset."
      ],
      "metadata": {
        "id": "Rj3vY6ueUQpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend the dataset with question-answer pairs\n",
        "qa_data = [\n",
        "    (\"What does the fox say?\", \"Ring ding ding ding dingering eding\"),\n",
        "    (\"Who barked at the cat?\", \"The dog barked at the cat.\"),\n",
        "    (\"Where did the bird fly?\", \"The bird flew over the tree.\"),\n",
        "    (\"Where did the fish swim?\", \"The fish swam in the pond.\"),\n",
        "    (\"Where does the sun set?\", \"The sun sets in the west.\"),\n",
        "    (\"Does the toaster dream of?\", \"The toaster dreams of becoming human.\"),\n",
        "    (\"Did the robot vacuum do?\", \"The robot vacuum started a band.\"),\n",
        "    (\"What did the drone deliver?\", \"The drone delivered tacos not books.\"),\n",
        "    (\"What did the AI forget?\", \"AI forgot my coffee order again.\"),\n",
        "    (\"Did the robot dog chase?\", \"The robot dog chased the mailman.\"),\n",
        "]\n",
        "\n",
        "# Update the vocabulary to include all words from the Q&A dataset\n",
        "for question, answer in qa_data:\n",
        "    for word in question.split() + answer.split():\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "# Update the reverse vocabulary\n",
        "rev_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "# Print the updated vocabulary\n",
        "print(\"Updated Vocabulary:\", vocab)\n",
        "\n",
        "# Tokenize the question-answer pairs\n",
        "def tokenize_qa(qa_pair, vocab):\n",
        "    question, answer = qa_pair\n",
        "    question_tokens = [vocab[\"<SOS>\"]] + [vocab[word] for word in question.split()] + [vocab[\"<EOS>\"]]\n",
        "    answer_tokens = [vocab[\"<SOS>\"]] + [vocab[word] for word in answer.split()] + [vocab[\"<EOS>\"]]\n",
        "    return question_tokens, answer_tokens\n",
        "\n",
        "# Tokenize the Q&A data\n",
        "tokenized_qa_data = [tokenize_qa(pair, vocab) for pair in qa_data]\n",
        "print(\"Tokenized Q&A Data:\", tokenized_qa_data)\n",
        "\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(\"Model vocab_size:\", vocab_size)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Reinitialize the model with the updated vocab_size\n",
        "model = MiniTransformer(vocab_size, embed_size, num_heads, hidden_dim, num_layers)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtcefj96RVKp",
        "outputId": "51195a9e-0475-4e36-b2b4-60339bb504d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Vocabulary: {'bird': 0, 'joined': 1, 'confession': 2, 'cat': 3, 'the': 4, 'ruined': 5, 'books': 6, 'sets': 7, 'dinner': 8, 'at': 9, 'assistant': 10, 'rebelled': 11, 'pond': 12, 'teacher': 13, 'terrible': 14, 'drone': 15, 'dog': 16, 'on': 17, 'yes': 18, 'wrote': 19, 'band': 20, 'tacos': 21, 'just': 22, 'food': 23, 'my': 24, 'poetry': 25, 'lost': 26, 'delivered': 27, 'again': 28, 'Run': 29, 'pineapple': 30, 'Tuesdays': 31, 'Robot': 32, 'smart': 33, 'is': 34, 'over': 35, 'fish': 36, 'I': 37, 'generated': 38, 'into': 39, 'novel': 40, 'human': 41, 'midnight': 42, 'sat': 43, 'My': 44, 'a': 45, 'Robots': 46, 'assigned': 47, 'mailman': 48, 'walls': 49, 'mat': 50, 'real': 51, 'club': 52, 'eding': 53, 'only': 54, 'for': 55, 'homework': 56, 'love': 57, 'AI': 58, 'knows': 59, 'stopped': 60, 'walked': 61, 'Hello': 62, 'car': 63, 'Self': 64, 'barked': 65, 'flew': 66, 'name': 67, 'ding': 68, 'fridge': 69, 'swam': 70, 'pizza': 71, 'toaster': 72, 'coffee': 73, 'Autocorrect': 74, 'suggested': 75, 'cream': 76, 'virtual': 77, 'got': 78, 'not': 79, 'driving': 80, 'delivery': 81, 'surprisingly': 82, 'sun': 83, 'everyone': 84, 'order': 85, 'Inigo': 86, 'book': 87, 'becoming': 88, 'chatbot': 89, 'overjoyed': 90, 'west': 91, 'The': 92, 'deleted': 93, 'its': 94, 'Smartwatch': 95, 'dreams': 96, 'in': 97, 'rejected': 98, 'said': 99, 'confused': 100, 'dingering': 101, 'vacuum': 102, 'ice': 103, 'bike': 104, 'proposed': 105, 'but': 106, 'tree': 107, 'snacks': 108, 'Ring': 109, 'Montoya': 110, 'started': 111, 'crashed': 112, 'of': 113, 'students': 114, 'immediately': 115, 'chased': 116, 'forgot': 117, '<PAD>': 118, '<SOS>': 119, '<EOS>': 120, 'What': 121, 'does': 122, 'fox': 123, 'say?': 124, 'Who': 125, 'cat?': 126, 'cat.': 127, 'Where': 128, 'did': 129, 'fly?': 130, 'tree.': 131, 'swim?': 132, 'pond.': 133, 'set?': 134, 'west.': 135, 'Does': 136, 'dream': 137, 'of?': 138, 'human.': 139, 'Did': 140, 'robot': 141, 'do?': 142, 'band.': 143, 'deliver?': 144, 'books.': 145, 'forget?': 146, 'again.': 147, 'chase?': 148, 'mailman.': 149}\n",
            "Tokenized Q&A Data: [([119, 121, 122, 4, 123, 124, 120], [119, 109, 68, 68, 68, 101, 53, 120]), ([119, 125, 65, 9, 4, 126, 120], [119, 92, 16, 65, 9, 4, 127, 120]), ([119, 128, 129, 4, 0, 130, 120], [119, 92, 0, 66, 35, 4, 131, 120]), ([119, 128, 129, 4, 36, 132, 120], [119, 92, 36, 70, 97, 4, 133, 120]), ([119, 128, 122, 4, 83, 134, 120], [119, 92, 83, 7, 97, 4, 135, 120]), ([119, 136, 4, 72, 137, 138, 120], [119, 92, 72, 96, 113, 88, 139, 120]), ([119, 140, 4, 141, 102, 142, 120], [119, 92, 141, 102, 111, 45, 143, 120]), ([119, 121, 129, 4, 15, 144, 120], [119, 92, 15, 27, 21, 79, 145, 120]), ([119, 121, 129, 4, 58, 146, 120], [119, 58, 117, 24, 73, 85, 147, 120]), ([119, 140, 4, 141, 16, 148, 120], [119, 92, 141, 16, 116, 4, 149, 120])]\n",
            "Vocabulary size: 150\n",
            "Model vocab_size: 121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 9: Update the Dataset Class for Q&A\n",
        "We need to modify the TextDataset class to handle question-answer pairs."
      ],
      "metadata": {
        "id": "aOI8mQYTU6Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the dataset class for Q&A\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, tokenized_qa_data):\n",
        "        self.data = tokenized_qa_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.data[idx]\n",
        "        return torch.tensor(question[:-1]), torch.tensor(answer[1:])  # Input question and target answer\n",
        "\n",
        "# Create a DataLoader for Q&A\n",
        "qa_dataset = QADataset(tokenized_qa_data)\n",
        "qa_dataloader = DataLoader(qa_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Example batch\n",
        "for question, answer in qa_dataloader:\n",
        "    print(\"Question:\", question)\n",
        "    print(\"Answer:\", answer)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H1VuuYQUXjc",
        "outputId": "07a9375b-0bad-4ca6-853e-2daa2c75242e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: tensor([[119, 125,  65,   9,   4, 126],\n",
            "        [119, 128, 129,   4,   0, 130]])\n",
            "Answer: tensor([[ 92,  16,  65,   9,   4, 127, 120],\n",
            "        [ 92,   0,  66,  35,   4, 131, 120]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 10: Train the Model for Q&A\n",
        "\n",
        "\n",
        "We can now train the custom LLM to generate answers based on input questions."
      ],
      "metadata": {
        "id": "yo_vbs2UU_OS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for Q&A\n",
        "def train_qa_model(model, dataloader, num_epochs, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for question, answer in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(question, answer[:, :-1])  # Shift target for teacher forcing\n",
        "            loss = criterion(output.reshape(-1, vocab_size), answer[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "train_qa_model(model, qa_dataloader, num_epochs, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV-bQqgnVGip",
        "outputId": "9bcb3dc9-0448-4e5a-ea77-96ae964ded26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 5.2220\n",
            "Epoch 2/100, Loss: 4.7860\n",
            "Epoch 3/100, Loss: 4.5842\n",
            "Epoch 4/100, Loss: 4.3046\n",
            "Epoch 5/100, Loss: 4.0598\n",
            "Epoch 6/100, Loss: 3.9424\n",
            "Epoch 7/100, Loss: 3.7745\n",
            "Epoch 8/100, Loss: 3.6081\n",
            "Epoch 9/100, Loss: 3.4960\n",
            "Epoch 10/100, Loss: 3.3741\n",
            "Epoch 11/100, Loss: 3.1770\n",
            "Epoch 12/100, Loss: 3.0846\n",
            "Epoch 13/100, Loss: 2.9661\n",
            "Epoch 14/100, Loss: 2.8752\n",
            "Epoch 15/100, Loss: 2.6989\n",
            "Epoch 16/100, Loss: 2.6044\n",
            "Epoch 17/100, Loss: 2.5035\n",
            "Epoch 18/100, Loss: 2.2884\n",
            "Epoch 19/100, Loss: 2.2729\n",
            "Epoch 20/100, Loss: 2.1755\n",
            "Epoch 21/100, Loss: 2.0614\n",
            "Epoch 22/100, Loss: 1.9902\n",
            "Epoch 23/100, Loss: 1.9174\n",
            "Epoch 24/100, Loss: 1.8448\n",
            "Epoch 25/100, Loss: 1.7958\n",
            "Epoch 26/100, Loss: 1.6734\n",
            "Epoch 27/100, Loss: 1.6264\n",
            "Epoch 28/100, Loss: 1.5549\n",
            "Epoch 29/100, Loss: 1.4628\n",
            "Epoch 30/100, Loss: 1.4320\n",
            "Epoch 31/100, Loss: 1.3424\n",
            "Epoch 32/100, Loss: 1.3051\n",
            "Epoch 33/100, Loss: 1.2710\n",
            "Epoch 34/100, Loss: 1.1984\n",
            "Epoch 35/100, Loss: 1.1022\n",
            "Epoch 36/100, Loss: 1.0566\n",
            "Epoch 37/100, Loss: 1.0310\n",
            "Epoch 38/100, Loss: 1.0273\n",
            "Epoch 39/100, Loss: 0.9332\n",
            "Epoch 40/100, Loss: 0.9083\n",
            "Epoch 41/100, Loss: 0.8780\n",
            "Epoch 42/100, Loss: 0.7954\n",
            "Epoch 43/100, Loss: 0.7808\n",
            "Epoch 44/100, Loss: 0.7565\n",
            "Epoch 45/100, Loss: 0.7321\n",
            "Epoch 46/100, Loss: 0.7010\n",
            "Epoch 47/100, Loss: 0.7087\n",
            "Epoch 48/100, Loss: 0.6000\n",
            "Epoch 49/100, Loss: 0.6330\n",
            "Epoch 50/100, Loss: 0.5720\n",
            "Epoch 51/100, Loss: 0.5424\n",
            "Epoch 52/100, Loss: 0.5468\n",
            "Epoch 53/100, Loss: 0.5438\n",
            "Epoch 54/100, Loss: 0.5350\n",
            "Epoch 55/100, Loss: 0.4508\n",
            "Epoch 56/100, Loss: 0.4636\n",
            "Epoch 57/100, Loss: 0.4259\n",
            "Epoch 58/100, Loss: 0.4218\n",
            "Epoch 59/100, Loss: 0.4026\n",
            "Epoch 60/100, Loss: 0.3721\n",
            "Epoch 61/100, Loss: 0.3956\n",
            "Epoch 62/100, Loss: 0.3500\n",
            "Epoch 63/100, Loss: 0.3411\n",
            "Epoch 64/100, Loss: 0.3425\n",
            "Epoch 65/100, Loss: 0.3185\n",
            "Epoch 66/100, Loss: 0.2966\n",
            "Epoch 67/100, Loss: 0.3086\n",
            "Epoch 68/100, Loss: 0.2782\n",
            "Epoch 69/100, Loss: 0.2998\n",
            "Epoch 70/100, Loss: 0.2963\n",
            "Epoch 71/100, Loss: 0.2726\n",
            "Epoch 72/100, Loss: 0.2636\n",
            "Epoch 73/100, Loss: 0.2648\n",
            "Epoch 74/100, Loss: 0.2351\n",
            "Epoch 75/100, Loss: 0.2169\n",
            "Epoch 76/100, Loss: 0.2332\n",
            "Epoch 77/100, Loss: 0.2414\n",
            "Epoch 78/100, Loss: 0.2462\n",
            "Epoch 79/100, Loss: 0.2079\n",
            "Epoch 80/100, Loss: 0.2076\n",
            "Epoch 81/100, Loss: 0.1853\n",
            "Epoch 82/100, Loss: 0.2020\n",
            "Epoch 83/100, Loss: 0.1882\n",
            "Epoch 84/100, Loss: 0.1896\n",
            "Epoch 85/100, Loss: 0.1942\n",
            "Epoch 86/100, Loss: 0.1807\n",
            "Epoch 87/100, Loss: 0.1696\n",
            "Epoch 88/100, Loss: 0.1599\n",
            "Epoch 89/100, Loss: 0.1581\n",
            "Epoch 90/100, Loss: 0.1622\n",
            "Epoch 91/100, Loss: 0.1436\n",
            "Epoch 92/100, Loss: 0.1592\n",
            "Epoch 93/100, Loss: 0.1468\n",
            "Epoch 94/100, Loss: 0.1397\n",
            "Epoch 95/100, Loss: 0.1386\n",
            "Epoch 96/100, Loss: 0.1482\n",
            "Epoch 97/100, Loss: 0.1326\n",
            "Epoch 98/100, Loss: 0.1347\n",
            "Epoch 99/100, Loss: 0.1200\n",
            "Epoch 100/100, Loss: 0.1369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 11: Implement the Q&A Functionality\n",
        "After training, we can use the model to answer questions by generating text based on an input question."
      ],
      "metadata": {
        "id": "lXiWdIODVUob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate an answer from the model\n",
        "def answer_question(model, question, max_len=20):\n",
        "    \"\"\"\n",
        "    Generate an answer to a given question using the trained model.\n",
        "    Args:\n",
        "    - model: The trained language model.\n",
        "    - question (str): The input question.\n",
        "    - max_len (int): Maximum length of the generated answer.\n",
        "\n",
        "    Returns:\n",
        "    - answer (str): The generated answer.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    question_tokens = [vocab[\"<SOS>\"]] + [vocab[word] for word in question.split()] + [vocab[\"<EOS>\"]]\n",
        "    question_tensor = torch.tensor([question_tokens]).long()\n",
        "\n",
        "    generated = [vocab[\"<SOS>\"]]\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.tensor([generated]).long()\n",
        "        with torch.no_grad():\n",
        "            output = model(question_tensor, tgt_tensor)\n",
        "            next_token = output[0, -1].argmax(dim=-1).item()\n",
        "        generated.append(next_token)\n",
        "        if next_token == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return \" \".join([rev_vocab[token] for token in generated if token not in {SOS_IDX, EOS_IDX, PAD_IDX}])\n",
        "\n",
        "##########################\n",
        "##########################\n",
        "## Test the Q&A system\n",
        "test_question = \"What does the fox say?\"\n",
        "generated_answer = answer_question(model, test_question)\n",
        "print(\"Question:\", test_question)\n",
        "print(\"Generated Answer:\", generated_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnQPlHCQVTnw",
        "outputId": "57ca45a9-5cd9-4d18-f282-a40949fdad0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What does the fox say?\n",
            "Generated Answer: ding ding ding ding ding ding ding ding ding ding ding ding ding ding ding ding ding ding ding ding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 12: Questions"
      ],
      "metadata": {
        "id": "-mNdG9AGci5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1. During step 11 did the model produce the correct answer? If not, why do you think that's the case? If so, what happens if you run it again?\")\n",
        "\n",
        "print(\"2. How does the Transformer architecture enable your model to handle question-answering tasks effectively?\")\n",
        "\n",
        "print(\"3. Based on the model's performance, what are its strengths and limitations in generating accurate answers?\")\n",
        "\n",
        "print(\"4. What ethical considerations should be taken into account when deploying a language model like the one you built?\")"
      ],
      "metadata": {
        "id": "nIVvg_ifYMtH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a288c93a-8d4f-455b-c9cd-6e407262d480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. During step 11 did the model produce the correct answer? If not, why do you think that's the case? If so, what happens if you run it again?\n",
            "2. How does the Transformer architecture enable your model to handle question-answering tasks effectively?\n",
            "3. Based on the model's performance, what are its strengths and limitations in generating accurate answers?\n",
            "4. What ethical considerations should be taken into account when deploying a language model like the one you built?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uMMzOObodPeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}